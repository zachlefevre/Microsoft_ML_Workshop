{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a jupyter notebook. You can selectively run any cell by selecting the cell and clicking **run**  \n",
    "(optional) Install Jupyter with:  \n",
    "```pip install jupyter```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install NumPy:  \n",
    "    ```pip install numpy```  \n",
    "Install Keras:  \n",
    "    ```pip install keras```  \n",
    "Install Matplotlib:\n",
    "    ```pip install matplotlib```  \n",
    "Install CNTK:  \n",
    "    1) Go to [The CNTK docs](https://docs.microsoft.com/en-us/cognitive-toolkit/setup-windows-python?tabs=cntkpy22)  \n",
    "    2) Find the library which matches your python installation, and computer architecture  \n",
    "    3) ```pip install <given url>```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open **cmd** and run **python**.  \n",
    "Type ```import keras```  \n",
    "You will receive errors. This is okay, we generating *%userprofile%\\\\.keras\\\\keras.json*  \n",
    "Open *%userprofile%\\\\.keras\\\\keras.json* and change the *backend* value to *cntk*  \n",
    "In your python console type ```import keras```. You should not receive errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import keras, matplotlib, and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n",
      "c:\\users\\zachl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\cntk_backend.py:19: UserWarning: CNTK backend warning: GPU is not detected. CNTK's CPU version is not fully optimized,please run with GPU to get better performance.\n",
      "  'CNTK backend warning: GPU is not detected. '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPool2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose a random number in the training data and display it with imshow.  \n",
    "Notice that I am graphing the X_train member to see the handwritten digit. Y_train is the corrsponding label to X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+IGg/h9IlEwmT1aenAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21ca8c2ec50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "toShow = np.random.randint(len(X_train))\n",
    "plt.imshow(X_train[toShow], cmap='Greys')\n",
    "plt.show()\n",
    "print(Y_train[toShow])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the X_train member is simply printed, The console will display a 28 x 28 array whose numbers are between 0 and 255.  \n",
    "0 represents the whitest white and 255 represents the blackest black. Any other number is some gray in between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0  46 125 165 197 231 254 254 255 207 165 128  75   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  68 244 254 254 254 254 254 254 232 254 254 254 253 241 154\n",
      "   35   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 245 254 244 160  77  26  26  26  17  26  26  26  26  70 251\n",
      "  220  37   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 198 254 246 139  55  10   0   0   0   0   0   0   0 125 254\n",
      "  254 199   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  16 191 251 254 254 215 139 112  60  19   0   0   0 169 254\n",
      "  222 222  39   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0  60 141 187 158 248 254 254 242 145  61  76 232 228\n",
      "   68  48  30   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  27  80 163 184 254 254 254 254 126\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1  47 217 254 254 228\n",
      "  142  10   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  39 230 231  75 208\n",
      "  254 225  70   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 168 254 144   0   9\n",
      "   50 204 241  96   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  33 231 196  13   0   0\n",
      "    0  36 248 247  47   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 140 254 109   0   0   0\n",
      "    0   0  56 228 202   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  22 234 198  16   0   0   0\n",
      "    0   0   0 190 254   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  59 254 133   0   0   0   0\n",
      "    0   0   0 162 254   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  59 254 133   0   0   0   0\n",
      "    0   0  26 252 254   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  59 254 183   0   0   0   0\n",
      "    0   9 107 254 203   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  29 238 249  96   0   0   0\n",
      "    9 137 254 247  50   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 168 249 233 172 117 139\n",
      "  215 254 241 100   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 105 246 254 254 254\n",
      "  254 233 104   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  39 164 164 164\n",
      "   83  37   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[toShow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin Setting up the model parameters with quantities that we alrady know.  \n",
    "Each image is 28x28, and there are 10 possible classes (labels): 0 .. 9  \n",
    "The batch size is a hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "img_colors = 1\n",
    "num_classes = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *image_data_format* attribute in the keras.json file specifies where the number of colors is represented in the image shape.  \n",
    "If *channels_first* is the value, then each image is an array of shape (img_colors, img_rows, img_cols).  \n",
    "If *channels_last* is the value, then each image is an array of shape (img_rows, img_cols, img_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_colors, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_colors, img_rows, img_cols)\n",
    "    img_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, img_colors)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, img_colors) \n",
    "    img_shape = (img_cols, img_rows, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure that our dataset is a sequence of floats, and that each number is between 0 and 1.  \n",
    "We are scaling our data set to be [0,1] inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out a training sample to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.18039216]\n",
      "  [ 0.49019608]\n",
      "  [ 0.64705884]\n",
      "  [ 0.77254903]\n",
      "  [ 0.90588236]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 1.        ]\n",
      "  [ 0.81176472]\n",
      "  [ 0.64705884]\n",
      "  [ 0.50196081]\n",
      "  [ 0.29411766]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.26666668]\n",
      "  [ 0.95686275]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.90980393]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99215686]\n",
      "  [ 0.94509804]\n",
      "  [ 0.60392159]\n",
      "  [ 0.13725491]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.96078432]\n",
      "  [ 0.99607843]\n",
      "  [ 0.95686275]\n",
      "  [ 0.627451  ]\n",
      "  [ 0.3019608 ]\n",
      "  [ 0.10196079]\n",
      "  [ 0.10196079]\n",
      "  [ 0.10196079]\n",
      "  [ 0.06666667]\n",
      "  [ 0.10196079]\n",
      "  [ 0.10196079]\n",
      "  [ 0.10196079]\n",
      "  [ 0.10196079]\n",
      "  [ 0.27450982]\n",
      "  [ 0.98431373]\n",
      "  [ 0.86274511]\n",
      "  [ 0.14509805]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.7764706 ]\n",
      "  [ 0.99607843]\n",
      "  [ 0.96470588]\n",
      "  [ 0.54509807]\n",
      "  [ 0.21568628]\n",
      "  [ 0.03921569]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.49019608]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.78039217]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.0627451 ]\n",
      "  [ 0.74901962]\n",
      "  [ 0.98431373]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.84313726]\n",
      "  [ 0.54509807]\n",
      "  [ 0.43921569]\n",
      "  [ 0.23529412]\n",
      "  [ 0.07450981]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.66274512]\n",
      "  [ 0.99607843]\n",
      "  [ 0.87058824]\n",
      "  [ 0.87058824]\n",
      "  [ 0.15294118]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.23529412]\n",
      "  [ 0.5529412 ]\n",
      "  [ 0.73333335]\n",
      "  [ 0.61960787]\n",
      "  [ 0.97254902]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.94901961]\n",
      "  [ 0.56862748]\n",
      "  [ 0.23921569]\n",
      "  [ 0.29803923]\n",
      "  [ 0.90980393]\n",
      "  [ 0.89411765]\n",
      "  [ 0.26666668]\n",
      "  [ 0.1882353 ]\n",
      "  [ 0.11764706]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.10588235]\n",
      "  [ 0.3137255 ]\n",
      "  [ 0.63921571]\n",
      "  [ 0.72156864]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.49411765]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.00392157]\n",
      "  [ 0.18431373]\n",
      "  [ 0.8509804 ]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.89411765]\n",
      "  [ 0.55686277]\n",
      "  [ 0.03921569]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.15294118]\n",
      "  [ 0.90196079]\n",
      "  [ 0.90588236]\n",
      "  [ 0.29411766]\n",
      "  [ 0.81568629]\n",
      "  [ 0.99607843]\n",
      "  [ 0.88235295]\n",
      "  [ 0.27450982]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.65882355]\n",
      "  [ 0.99607843]\n",
      "  [ 0.56470591]\n",
      "  [ 0.        ]\n",
      "  [ 0.03529412]\n",
      "  [ 0.19607843]\n",
      "  [ 0.80000001]\n",
      "  [ 0.94509804]\n",
      "  [ 0.3764706 ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.12941177]\n",
      "  [ 0.90588236]\n",
      "  [ 0.76862746]\n",
      "  [ 0.05098039]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.14117648]\n",
      "  [ 0.97254902]\n",
      "  [ 0.96862745]\n",
      "  [ 0.18431373]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.54901963]\n",
      "  [ 0.99607843]\n",
      "  [ 0.42745098]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.21960784]\n",
      "  [ 0.89411765]\n",
      "  [ 0.79215688]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.08627451]\n",
      "  [ 0.91764706]\n",
      "  [ 0.7764706 ]\n",
      "  [ 0.0627451 ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.74509805]\n",
      "  [ 0.99607843]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.23137255]\n",
      "  [ 0.99607843]\n",
      "  [ 0.52156866]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.63529414]\n",
      "  [ 0.99607843]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.23137255]\n",
      "  [ 0.99607843]\n",
      "  [ 0.52156866]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.10196079]\n",
      "  [ 0.98823529]\n",
      "  [ 0.99607843]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.23137255]\n",
      "  [ 0.99607843]\n",
      "  [ 0.71764708]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.03529412]\n",
      "  [ 0.41960785]\n",
      "  [ 0.99607843]\n",
      "  [ 0.79607844]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.11372549]\n",
      "  [ 0.93333334]\n",
      "  [ 0.97647059]\n",
      "  [ 0.3764706 ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.03529412]\n",
      "  [ 0.53725493]\n",
      "  [ 0.99607843]\n",
      "  [ 0.96862745]\n",
      "  [ 0.19607843]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.65882355]\n",
      "  [ 0.97647059]\n",
      "  [ 0.9137255 ]\n",
      "  [ 0.67450982]\n",
      "  [ 0.45882353]\n",
      "  [ 0.54509807]\n",
      "  [ 0.84313726]\n",
      "  [ 0.99607843]\n",
      "  [ 0.94509804]\n",
      "  [ 0.39215687]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.41176471]\n",
      "  [ 0.96470588]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.99607843]\n",
      "  [ 0.9137255 ]\n",
      "  [ 0.40784314]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.15294118]\n",
      "  [ 0.64313728]\n",
      "  [ 0.64313728]\n",
      "  [ 0.64313728]\n",
      "  [ 0.32549021]\n",
      "  [ 0.14509805]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]\n",
      "\n",
      " [[ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]\n",
      "  [ 0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[toShow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert our Y_train members from scalars to one_hot representations of those numbers.  \n",
    "Ex:  \n",
    "    7 becomes [0,0,0,0,0,0,0,1,0,0]  \n",
    "    2 becomes [0,0,1,0,0,0,0,0,0,0]  \n",
    "Keras has a convenient function to doing exactly this.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "Y_train = np_utils.to_categorical(Y_train, num_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, num_classes)\n",
    "print(Y_train[toShow])\n",
    "print(Y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, kernel_size = (3,3),\n",
    "        activation='relu',\n",
    "        input_shape= img_shape))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the standards of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.categorical_crossentropy,\n",
    "                    optimizer = keras.optimizers.Adadelta(),\n",
    "                    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zachl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input120\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 655s - loss: 0.3504 - acc: 0.8939 - val_loss: 0.0871 - val_acc: 0.9736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21ca8c8a908>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,\n",
    "    Y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs = 1,\n",
    "    verbose = 1,\n",
    "    validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the quality of our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   64/10000 [..............................] - ETA: 46s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zachl\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\cntk\\core.py:361: UserWarning: your data is of type \"float64\", but your input variable (uid \"Input120\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 39s    \n",
      "loss:  0.0870785920255\n",
      "accuracy:  0.9736\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"loss: \", score[0])\n",
    "print(\"accuracy: \", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
