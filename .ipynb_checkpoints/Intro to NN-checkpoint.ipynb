{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of ML\n",
    "There are several different type of machine learning. The easiest two understand as a beginner are:\n",
    "1. supervised\n",
    "    - data is labelled.\n",
    "2. unsupervised\n",
    "    - data is unlabelled.\n",
    "\n",
    "### Data\n",
    "All types of machine learning require data. That data is a numerical vector. **EVERYTHING** in ML are vectors/matrices. Inputs, outputs, weights, scores, gradients, losses, etc\n",
    "* If the neural network is operating on images of handwitten digits which are of the size 28 x 28, then the input vector will be a single 784 length vector whose elements represent how dark each pixel is.\n",
    "* If the neural network is operating on 8 movie scores, then the input vector is an 8 length input vector where each element is the rating of one movie\n",
    "\n",
    "The more data you have, the better your NN will perform. Usually.\n",
    "* The more examples of images of numbers you have, the better your NN will do.\n",
    "* The more complex each training example is, the more likely you will overfit your NN to the training data.\n",
    "    * Intuitively, if the data is unecessarily complex then the NN will make assumptions based on those properties that it shouldn't. If my images of handwritten digits also contain color, and 60% of the images of '4' are blue, then our NN will assume that the more blue an image is, the more likely it is a '4'\n",
    "    * More complex data also means larger input vectors. If an image is 28 x 28 and there are 3 colors, then the input vector is of size 28 \\* 28 \\* 3. **The larger your input vector, the more likely you will overfit your data**. (**The more complex your data, the more likely you will overfit your data**.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = [1,2,3]\n",
    "m = [[1,2,3],\n",
    "     [4,5,6],\n",
    "     [7,8,9]]\n",
    "\n",
    "print(v)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is overfitting\n",
    "An overfit neural network is one which does very well classifying data it was trained on, but does poorly on new data.  \n",
    "* If our NN can classify with 99% accuracy an element of our training data set, but only performs with an accuracy of 50% on an image it has never seen, then our NN is overfit.\n",
    "    ##### How to prevent overfitting\n",
    "    We split our data set into two groups.\n",
    "    1. Training\n",
    "        * We train on this set (obviously). If our NN mistakes a '4' for an '8', then we change our NN to better classify this '4'.\n",
    "    2. Testing\n",
    "        * We **never** train on this set. If our NN mistakes a '4' for an '8', then we store that it got it wrong, and use this tally to calculate how well our NN did.\n",
    "        * As we train our NN, we will test that our scores for the testing set are not going down. The testing scores going down signifies that we are overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST training set and testing set from [here](https://pjreddie.com/projects/mnist-in-csv/), and put the .csv in the same directory as this notebook.  \n",
    "The MNIST dataset contains examples of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_data_file_ref = open(\"./mnist_train.csv\", 'r')\n",
    "training_data_set = training_data_file_ref.readlines();\n",
    "training_data_file_ref.close();\n",
    "\n",
    "testing_data_file_ref = open(\"./mnist_test.csv\", 'r')\n",
    "testing_data_set = testing_data_file_ref.readlines();\n",
    "testing_data_file_ref.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testing_data_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(training_data_set):\n",
    "    training_data_set[i] = np.asfarray(training_data_set[i].split(',')) #every example in the training set is a huuuuge string. Split it on the commas\n",
    "    \n",
    "for i, v in enumerate(testing_data_set):\n",
    "    testing_data_set[i] = np.asfarray(testing_data_set[i].split(','))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_example = training_data_set[888]\n",
    "print(\"There are\", len(training_data_set), \"images\")\n",
    "print(training_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any element of the training_data_set array is a 785 length array, where the 0th element is the label and the other 784 elements are a flattened image of a handwritten digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "label = training_example[0] #The first element is the label\n",
    "training_example = training_example[1:].reshape((28,28)) #convert it to an array of floats of size 28 x 28\n",
    "\n",
    "print(label)\n",
    "plt.imshow(training_example, cmap='Greys')\n",
    "plt.show()\n",
    "print(training_example.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down what our data set is:\n",
    "* A file containing 60,000 lines\n",
    "    * each line is a string\n",
    "        * each string is one training example, 785 characters long\n",
    "            * the first element of each training example is a label ('5', or '8', or some other label)\n",
    "                * the rest of the of the training example is a vector 784 characters long\n",
    "                     * each element of the vector represents the intensity of one pixel of a 28 x 28 image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "A NN is a pattern recognition device. A NN is used when the pattern being recognized is too complex to accomplish with traditional algorithms.\n",
    "* It trivial to recognize that 101 is 5 in binary. A NN is not necessary for this task.\n",
    "* It is simple to recognize a single image of a '5' and differentiate it from other numbers. A NN is not necessary for this task.\n",
    "* It is next to impossible to recognize thousands of images of '5's, and differentiate them from thousands of images of other numbers. A NN **is** necessary for this task.\n",
    "\n",
    "The output of the NN will be a vector representation of the \"confidence\" that the NN thinks an image matches the labels.\n",
    "* A NN which is 100% confident that it was given an image of '0' will output [1,0,0,0,0,0,0,0,0,0].\n",
    "* A NN which is 100% confident that it was given an image of '4' will output [0,0,0,0,1,0,0,0,0,0].\n",
    "* A NN which is 60% confident that it was given an image of '8', and 30% confident it was given an image of '1', and 10% confident that it was given an image of '3' will output [0, .3, 0, .1, 0, 0, 0, 0, .6, 0] \n",
    "\n",
    "**The output of a NN is a matrix of confidence scores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A single Neuron\n",
    "![Brain Neuron](imgs\\neuron_med.jpeg)\n",
    "\n",
    "A NN is a directed graph of neurons. A neuron is a node which has n inputs, 1 output, and some activation function.  \n",
    "All n inputs come into the neuron. Each input is weighted by some quantity, w1, ..., wn.  The weighted sum is calculated and passed to some activation function. The output of the activation function is the output of the neuron.\n",
    "![Single Neuron](imgs\\neuron_1.jpg)\n",
    "\n",
    "A 'vanilla' NN has 3 layers of neurons. The **input layer**, the **hidden layer**, and the **output layer**. Every neuron of the input layer is connected to every node of the hidden layer. Every node of the hidden layer is connected to every node of the output layer.\n",
    "\n",
    "![Artificial Neural Network](imgs\\ANN_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it all relates to matrices (optional)\n",
    "The weights can be represented as a matrix of weights  \n",
    "[w11, w21, w31]  \n",
    "[w12, w22, w32]  \n",
    "[w13, w23, w33]  \n",
    "where each wij is the weight from the ith node in the previous layer to the jth node in the current layer.  \n",
    "If we multiply this weight matrix by the output of the previous layer, then apply the activation function to the resulting vector, we will have the output of the current layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.matrix([[.2,.6,.3],[.4,.7,.9],[.2,.3,.8]]) # The weights of the connections from the previous layer to the current layer\n",
    "i = np.matrix([.1,.8,.3]).T                 # The output of the previous layer, aka the input of the current layer\n",
    "o = w.dot(i)# The product of these two matrices\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "The choice of activation function is very important. To mimick the way a neuron in the brain functions, we need something which is asymptotically limited to be between 0 and 1 for all inputs, and which will very quickly 'self-correct'. We will use the sigmoid function, which looks like this: \n",
    "![sigmoid function graph](imgs\\sigmoid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "activated_o = expit(o)\n",
    "print(activated_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activated_o is the final outut of this fake layer. This column vector would then be passed as input to the next layer where the process repeats.\n",
    "\n",
    "The number of inputs and outputs of a NN is evident in the problem. One of our images is a 784 length vector, therefore our NN will have 784 input neurons. The output of our NN is the classification of the image, i.e. '0', '1', ... '8', '9'.\n",
    "\n",
    "## NN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        self.input_to_hidden_weights = np.random.normal(0.0, pow(self.hidden_nodes, -0.5), (self.hidden_nodes, self.input_nodes))\n",
    "        self.hidden_to_output_weights = np.random.normal(0.0, pow(self.output_nodes, -0.5), (self.output_nodes, self.hidden_nodes))\n",
    "    \n",
    "    def activate(self, x):\n",
    "        return expit(x)\n",
    "    \n",
    "    def query(self, input_list):\n",
    "        #convert input_list into a 2d array\n",
    "        input = np.array(input_list, ndmin=2).T\n",
    "        \n",
    "        hidden_input_signal = np.dot(self.input_to_hidden_weights, input)\n",
    "        hidden_output_signal = self.activate(hidden_input_signal)\n",
    "        output_input_signal = np.dot(self.hidden_to_output_weights, hidden_output_signal)\n",
    "        output_output_signal = self.activate(output_input_signal);\n",
    "        \n",
    "        return output_output_signal\n",
    "        \n",
    "    def shape(self):\n",
    "        return [self.input_nodes, self.hidden_nodes, self.output_nodes];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 784\n",
    "num_hidden = 100\n",
    "num_output = 10\n",
    "\n",
    "NN = NeuralNetwork(num_input, num_hidden, num_output);\n",
    "\n",
    "\n",
    "training_example = training_data_set[0]\n",
    "print(\"correct answer: \", training_example[0])\n",
    "print(NN.query(training_example[1:]))\n",
    "print(\"NN think: \", np.argmax(NN.query(training_example[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our NN has given a score! It is entirely completely wrong, and isn't normalized to be percentages, but it queried and that's all that really matters. Our NN is so far entirely untrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Training a NN is it's own art, philosophy, field of science and religion. I will give the basics.\n",
    "\n",
    "We must calculate how wrong we were.\n",
    "* The correct classification for '5' is [0,0,0,0,0,1,0,0,0,0], but our NN was entirely wrong. It gave the above vector. We must calculate the distance between these vectors. This distance however only tells us how incorrect the final output layer was, it tells us nothing about the hidden layer and the weights to it.\n",
    "    * We use **gradient descent** to attribute the responsibility of the final scores to each weight in the network\n",
    "        * If a weight is very high, like .8, and our output is very wrong, then we need to change the weight .8 much more than a very small weight.\n",
    "    * Once we have attributed responsibilit to certain weights, we perform **back propagation** to go backwards through the network and update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork2:\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.input_to_hidden_weights = np.random.normal(0.0, pow(self.hidden_nodes, -0.5), (self.hidden_nodes, self.input_nodes))\n",
    "        self.hidden_to_output_weights = np.random.normal(0.0, pow(self.output_nodes, -0.5), (self.output_nodes, self.hidden_nodes))\n",
    "    \n",
    "    def activate(self, x):\n",
    "        return expit(x)\n",
    "\n",
    "    def train(self, input_list, target_list):\n",
    "        #convert input_list into a 2d array\n",
    "        input = np.array(input_list, ndmin=2).T\n",
    "        #convert target_list into a 2d array\n",
    "        target = np.array(target_list, ndmin=2).T\n",
    "        \n",
    "        #calculate signals in and out of hidden layers\n",
    "        hidden_input_signal = np.dot(self.input_to_hidden_weights, input)\n",
    "        hidden_output_signal = self.activate(hidden_input_signal)\n",
    "        \n",
    "        \n",
    "        output_input_signal = np.dot(self.hidden_to_output_weights, hidden_output_signal)\n",
    "        output_output_signal = self.activate(output_input_signal);\n",
    "        \n",
    "        output_error = target - output_output_signal\n",
    "        \n",
    "        hidden_error = np.dot(self.hidden_to_output_weights.T, output_error)\n",
    "        \n",
    "        self.hidden_to_output_weights += self.learning_rate * np.dot((output_error * output_output_signal * (1.0 - output_output_signal)), np.transpose(hidden_output_signal))\n",
    "        self.input_to_hidden_weights += self.learning_rate * np.dot((hidden_error * hidden_output_signal * (1-hidden_output_signal)), np.transpose(input))\n",
    "    \n",
    "    \n",
    "    def query(self, input_list):\n",
    "        #convert input_list into a 2d array\n",
    "        input = np.array(input_list, ndmin=2).T\n",
    "        \n",
    "        hidden_input_signal = np.dot(self.input_to_hidden_weights, input)\n",
    "        hidden_output_signal = self.activate(hidden_input_signal)\n",
    "        output_input_signal = np.dot(self.hidden_to_output_weights, hidden_output_signal)\n",
    "        output_output_signal = self.activate(output_input_signal);\n",
    "        \n",
    "        return output_output_signal\n",
    "        \n",
    "    def shape(self):\n",
    "        return [self.input_nodes, self.hidden_nodes, self.output_nodes];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "num_input = 784\n",
    "num_hidden = 100\n",
    "num_output = 10\n",
    "learning_rate = 0.03\n",
    "NN = NeuralNetwork2(num_input, num_hidden, num_output, learning_rate);\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"epoch: \", epoch)\n",
    "    for example in training_data_set:\n",
    "        label = np.zeros(10) + 0.01\n",
    "        label[int(example[0])] = .99\n",
    "        training_example = example[1:].copy()\n",
    "        training_example /= 255.\n",
    "        training_example *= .99\n",
    "        training_example += 0.01\n",
    "        NN.train(training_example, label)\n",
    "idx = 0\n",
    "numWrong = 0\n",
    "for example in testing_data_set:\n",
    "    label = np.zeros(10) + 0.01\n",
    "    label[int(example[0])] = .99\n",
    "    testing_example = example[1:].copy()\n",
    "    testing_example /= 255.\n",
    "    testing_example *= .99\n",
    "    testing_example += 0.01\n",
    "    if(np.argmax(NN.query(testing_example)) != int(example[0])) :\n",
    "       # print(\"correct: \", example[0], \"| estimated: \", np.argmax(NN.query(testing_example)), \"idx: \", idx)\n",
    "        numWrong += 1\n",
    "    idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1 - numWrong/len(testing_data_set))\n",
    "print(\"Number wrong: \", numWrong)\n",
    "print(\"Out of: \", len(testing_data_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Parameters\n",
    "A hyper parameter is anything that you \"choose\" for your neural network. The input size is inherent in the data, the output size is inherent in the classes, but the number of hidden layers is a hyper parameter (A NN with more than one hidden layer is a deep neural network), as is the size of those hidden layers and the learning rate (the learning rate is how far down the weight gradient to travel when performing gradient descent).\n",
    "\n",
    "##### Deep Neural Networks\n",
    "Each successive layer of a neural network is capable of discovering more complex relationships.\n",
    "* Say a neural network is tasked with finding the relationship of an image of a house, and how much that house sells for. If the first layer is capable of finding how much each window is worth and how much the size of the yard influences the cost of the house, then the next successive layer will be capable of finding the subtle relationship between number of windows and size of yard and how that influences the cost of the house. The next layer can find how that relationship and some other subtle relationship affect the cost of the house. To construct an algorithm to quantify this without the help of a pattern recognizing neural network would be disgusting.\n",
    "* Each layer can understand more and more complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks in other forms\n",
    "Recurrent Neural Networks have cycles which are temporally dependent.\n",
    "![Recurrent Neural Network](imgs\\rnn.gif)\n",
    "This more closely resembles how the brain actually functions, however current training techniques (gradient descient, back propagation, etc) do not perform well on recurrent neural nets. A recurrent neural network allows the network to understand things happening in sequence. This is thanks to the loops backwards through the layers which are delayed by time.\n",
    "* If A happend then B will happen then C. As oppossed to if A happens then B and C will likely happen.\n",
    "\n",
    "Capsule Neural Networks have built in spatial recognizers. This allows the NN to process that it is viewing something from an angle that it has not been trained on. Other NNs can differentiate between objects from different viewing angles, but only if they are trained on those angles. The capsule network was announced just a few months ago by Geoffrey Hinton, the discoverer of the usefullness of backpropagation. **Neural Networks are useful because of Geoffrey Hinton**. (He has a coursera course I whole heartedly recommend).\n",
    "![Capsule Network](imgs\\capsule.png)\n",
    "\n",
    "Convolutional Neural Networks are specifically designed for images. The input is a 3 dimensional image, with the two spatial (x, y) dimensions and the channel dimensions (color). There is no flattening until the very end. Essentially the ConvoNet performs feature recognition by creating dozens of filters which are trained to recognize curves, textures, shapes, etc.. and then plugging that into a classifier neural network, like we just built.\n",
    "\n",
    "The convolutional neural network does not flatten out the image into a long vector, because doing so would destroy all spatial information known about the image, which isn't very conducive to shape recognition and whatnot.  \n",
    "The \"convolutional\" part of a convolutional network defines a network structure whereby you define, as a hyper parameter, the number of \"filters\" to create. Those filters will be small compared to the rest of the image. These filters are going to be trained to detect the textures and shapes and whatnot.  \n",
    "These filters walk through the entire image, creating a neuron on every possible space of the image. This creates a 1 dimensional \"activation map\", which represents how much the feature of the filter is represented by any portion of the image.\n",
    "* A curve detector will create an activation map of the original image that is \"hotter\" where a curve is present\n",
    "Multiple convolutional layers acting on the previous activation maps, each with their own filters, are constructed in order to detect more complex information.\n",
    "The final (very complicated) output of the convolutional layers (also called the feature extraction portion) is passed into a classifier, exactly like the hand written digit recognizer that we created above. Because the image is 3 dimensions, and each filter creates it's own activation map, the outputs of each layer are constantly 3 dimensional and our network itself operates in 3 dimensions. These 3 dimensions must be flattened before being passed to the classifier.\n",
    "![CNN](imgs\\CNN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Nets do not function like human brains and they can be targeted in an adverial attack. [Objects can be 3d printed with a determined classification from a NN](http://www.labsix.org/physical-objects-that-fool-neural-nets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended Further Studies:  \n",
    "    [Make Your Own Neural Network](https://www.amazon.com/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G)  \n",
    "    [The Nature of Code](http://natureofcode.com/)  \n",
    "    [Stanford CS 231 for Convolutional Neural Networks](http://cs231n.github.io/)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the NN we just created thinks the \"ideal\" images are, check out [my other github repository](https://github.com/zachlefevre/neuralNetFromScratch/blob/master/NN%20from%20scratch.ipynb). Each pixel represents how \"important\" that pixel is to classifying the image.\n",
    "* If the \"ideal\" '3' (also called the prototype of '3'), has a dark pixel, then an image being passed to the NN with a dark pixel in that same location counts towards the input being a '3'."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
